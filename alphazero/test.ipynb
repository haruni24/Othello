{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ray\n",
    "from tqdm import tqdm\n",
    "\n",
    "import othello\n",
    "from network import PVNet\n",
    "from MCTS import MCTS\n",
    "from buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@ray.remote(num_cpus=1, num_gpus=0)\n",
    "def selfplay(weights, num_mcts_simulations, dirichlet_alpha=0.35):\n",
    "    @dataclass\n",
    "    class Sample:\n",
    "        board: list\n",
    "        mcts_policy: list\n",
    "        player: int\n",
    "        reward: int\n",
    "\n",
    "    record = []\n",
    "    board = othello.get_initial_board()\n",
    "\n",
    "    network = PVNet()\n",
    "    network(othello.encode_state(board, othello.BLACK))\n",
    "    network.set_weights(weights)\n",
    "\n",
    "    mcts = MCTS(network, alpha=dirichlet_alpha)\n",
    "\n",
    "    current_player = othello.BLACK\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not othello.is_done(board):\n",
    "        mcts_policy = mcts.search(board, current_player, num_mcts_simulations)\n",
    "        if i < 10:\n",
    "            move = np.random.choice(range(othello.ACTION_SPACE), p=mcts_policy)\n",
    "\n",
    "        else:\n",
    "            move = np.random.choice(np.where(mcts_policy == mcts_policy.max())[0])\n",
    "\n",
    "        record.append(Sample(board.tolist(), mcts_policy.tolist(), current_player, None))\n",
    "\n",
    "        board = othello.step(board, move, current_player)\n",
    "        current_player = 3 - current_player\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    for sample in record:\n",
    "        sample.reward = othello.get_result(sample.board, sample.player)\n",
    "\n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 4\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(n_parallel_selfplay=20,\n",
    "        num_mcts_simulations=50):\n",
    "\n",
    "    ray.shutdown()\n",
    "    ray.init(num_cpus=num_cpus, num_gpus=1)\n",
    "\n",
    "    network = PVNet()\n",
    "    dummy_state = othello.encode_state(othello.get_initial_board(), 1)\n",
    "    network(dummy_state)\n",
    "\n",
    "    current_weights = ray.put(network.get_weights())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=0.0005)\n",
    "    \n",
    "    replay = ReplayBuffer(buffer_size=40000)\n",
    "\n",
    "    work_in_progresses = [\n",
    "        selfplay.remote(current_weights, num_mcts_simulations)\n",
    "        for _ in range(n_parallel_selfplay)]\n",
    "    \n",
    "    n = 0\n",
    "    while n <= 10000:\n",
    "        for _ in tqdm(range(4)):\n",
    "            finished, work_in_progresses = ray.wait(work_in_progresses, num_returns=1)\n",
    "            replay.add_record(ray.get(finished[0]))\n",
    "            work_in_progresses.extend([\n",
    "                selfplay.remote(current_weights, num_mcts_simulations)\n",
    "            ])\n",
    "            n += 1\n",
    "        \n",
    "        num_iters = 5 * (len(replay) // batch_size)        \n",
    "        for i in range(num_iters):\n",
    "            \n",
    "            boards, mcts_policy, rewards = replay.get_minibatch(batch_size=batch_size)\n",
    "            boards = torch.tensor(boards, dtype=torch.float32)\n",
    "            mcts_policy = torch.tensor(mcts_policy, dtype=torch.float32)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            network.train()\n",
    "            p_pred, v_pred = network(boards)\n",
    "            \n",
    "            value_loss = (rewards - v_pred).pow(2)\n",
    "            \n",
    "            policy_loss = -mcts_policy * torch.log(p_pred + 1e-4)\n",
    "            policy_loss = torch.sum(policy_loss, dim=1, keepdim=True)\n",
    "\n",
    "            loss = torch.mean(value_loss + policy_loss)\n",
    "            print(f\"loss: {loss.item()}\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        current_weights = ray.put(network.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 19:11:03,515\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "100%|██████████| 5/5 [00:41<00:00,  8.24s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(n_parallel_selfplay, num_mcts_simulations)\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 39\u001b[0m p_pred, v_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m value_loss \u001b[38;5;241m=\u001b[39m (rewards \u001b[38;5;241m-\u001b[39m v_pred)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     43\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmcts_policy \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(p_pred \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-4\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/BoardGames/Othello/alphazero/network.py:27\u001b[0m, in \u001b[0;36mPVNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m()\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m: x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
